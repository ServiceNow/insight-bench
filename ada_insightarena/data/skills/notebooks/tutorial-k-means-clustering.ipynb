{"cells":[{"metadata":{},"cell_type":"markdown","source":"![K_Means_Header](https://raw.githubusercontent.com/satishgunjal/images/master/K_Means_Header.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Index\n\n* [Introduction](#1)\n* [Uses](#2)\n* [Inner Working of K-Means Clustering](#3)\n* [Random Initialization Guidelines](#4)\n* [Choosing The Number of Clusters](#5)\n  - [Visualization](#6)\n  - [Elbow Method](#7)\n* [Advantages](#8)\n* [Disadvantages](#9)\n* [Python Example](#10)\n  - [Import The Library](#11)\n  - [Load Data](#12)\n  - [Understanding The Data](#13)\n  - [Choosing The Number of Clusters](#14)\n  - [Elbow Method](#15)\n  - [Compute K-Means Clustering](#16)\n  - [Visualization](#17)\n  - [Inner Working](#18)\n  - [Inner Working: GIF](#19) ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Introduction <a id =\"1\"></a>\nIf a dataset do not have any labels associated with it, then unsupervised algorithms are used to find some structure in it. These structures can be different types of data pattern or group of data. K-Means clustering is most commonly used unsupervised learning algorithm to find groups in unlabeled data. Here K represents the number of groups or clusters and the process of creating these groups is known as 'clustering', that why the name K-means clustering.\n\n\n# Uses <a id =\"2\"></a>\n* **Search engine**: Search engine, groups results together using clustering algorithm\n* **Customer segmentation**: K-mean clustering can be used to create customer clusters based on demographic information, geographical information and behavioral data.\n* **Social network analysis**: To find groups of people with specific interest to direct the personalized ads.\n* **Data center**: To organize the computer clusters in data center.\n* **Inventory management**: Create inventory clusters based on sales number and manufacturing capacity\n\n# Inner Working of K-Means Clustering <a id =\"3\"></a>\nK-means is often referred to as Lloyd’s algorithm. It is one of the most popular clustering algorithm. Refer below plot where there are two clusters (K=2) one is of red data points and another one of green data points.\n\n![Two_Cluster_Centroid_With_Datapoints](https://raw.githubusercontent.com/satishgunjal/images/master/Two_Cluster_Centroid_With_Datapoints.png)\n\nSo how does K-Means algorithm find the clusters of the data points without any label? Below steps will explain the inner working of K-Means algorithm. \n\n* First step is to finalize the number of clusters you want to identify in your data. This is the \"K\" in K-means clustering.\n* Now randomly initialize the points equal to the number of clusters K. 'Cluster Centroid' is the terminology used to refer these points.\n* Note that centroid means center point of given dataset, but initially these points are at random location, but at the end when K-Means algorithm will converge they will be at the center of their respective cluster.\n* Once cluster centroids are defined, K-means algorithm will go through each data point from given data and depending on that points closeness to cluster centroid, it will assign the data point to the cluster centroid. This is called as 'Assignment Step'.\n* In order to move the cluster centroids from random location to their respective group, K-means algorithm will find the mean of each data point assigned to the cluster centroid and move the respective centroid to the mean value location. This is called as 'Move Centroid Step'\n* Note that during 'Move Centroid Step' data points can get reassigned from one cluster to another as centroid position change.\n* Now repeat the assignment and move centroid steps till cluster centroid position don't change. K-means algorithm will converge when we get the unchanged position of cluster centroids. \n* Once K-means algorithm is converged, data point assigned to respective centroid will represent the respective cluster.\n* During cluster assignment step if we found a centroid who has no data point associated with it, then it's better to remove it.\n\n![Inner_Working_K_Means](https://raw.githubusercontent.com/satishgunjal/images/master/Inner_Working_K_Means.png)\n\nSince we have to randomly pick the cluster centroids, it's initialization may affect the final outcome of the clustering. In case our initialization is not correct, then K-Means algorithm may form a cluster with few points only. Such situation is referred as 'centroid random initialization trap' and it may cause algorithm to get stuck at local optima.\n\nCheck below plots, where for same dataset, we end up getting different clusters depending on initial position of cluster centroids. Gray color squares represent the initial positions of centroids and red, green and blue squares represent the final position of centroids.\n\n![Centroid_Random_Initialization_Trap](https://raw.githubusercontent.com/satishgunjal/images/master/Centroid_Random_Initialization_Trap.png)\n\n# Random Initialization Guidelines <a id =\"4\"></a>\nTo avoid random initialization trap, follow below guidelines for random initialization.\n\n* Number of cluster centroids should be less than number of training examples\n* To avoid local optima issue, try to do multiple random initialization of centroids. \n* Multiple random initialization technique is more effective when we have a small number of clusters.\n* Similarly for large number of clusters, few random initialization are sufficient\n\n# Choosing The Number of Clusters <a id =\"5\"></a>\nSo using random initialization we can avoid the local optima issue, but to choose how many clusters to look for in a given data we can use below methods.\n\n## Visualization <a id =\"6\"></a>\nTo find the number of clusters manually by data visualization is one of the most common method. Domain knowledge and proper understanding of given data also help to make more informed decisions. Since its manual exercise there is always a scope for ambiguous observations, in such cases we can also use 'Elbow Method'\n\n## Elbow Method <a id =\"7\"></a>\nIn Elbow method we run the K-Means algorithm multiple times over a loop, with an increasing number of cluster choice(say from 1 to 10) and then plotting a clustering score as a function of the number of clusters. Clustering score is nothing but sum of squared distances of samples to their closest cluster center. Elbow is the point on the plot where clustering score (distortion) slows down, and the value of cluster at that point gives us the optimum number of clusters to have. But sometimes we don't get clear elbow point on the plot, in such cases its very hard to finalize the number of clusters. \n\n![K_Means_Elbow_Method](https://raw.githubusercontent.com/satishgunjal/images/master/K_Means_Elbow_Method.png)\n\n\n# Advantages <a id =\"8\"></a>\n* One of the simplest algorithm to understand\n* Since it uses simple computations it is relatively efficient\n* Gives better results when there is less data overlapping\n\n# Disadvantages <a id =\"9\"></a>\n* Number of clusters need to be defined by user\n* Doesn't work well in case of overlapping data\n* Unable to handle the noisy data and outliers\n* Algorithm fails for non-linear data set","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Python Example <a id =\"10\"></a>\nNow we will use k-means clustering algorithm on [mall customer unlabeled data](https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python) to create groups of the customer based on annual spending and spending score assigned by the mall.\nWe are going to use sklearns library for it.\n\n## Import The Library <a id =\"11\"></a>\n* pandas: Used for data manipulation and analysis\n* numpy : Numpy is the core library for scientific computing in Python. It is used for working with arrays and matrices.\n* matplotlib : It’s plotting library, and we are going to use it for data visualization\n* KMeans: Sklearn library for K-Means clustering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Data <a id =\"12\"></a>\n* We are going to use ‘Mall_Customers.csv’ CSV file\n* Dataset contains 5 columns CustomerID, Gender, Age, Annual Income (k$), Spending Score (1-100)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#df = pd.read_csv('https://raw.githubusercontent.com/satishgunjal/datasets/master/Mall_Customers.csv')\ndf = pd.read_csv('/kaggle/input/customer-segmentation-tutorial-in-python/Mall_Customers.csv')\nprint(\"Shape of the data= \", df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Understanding The Data <a id =\"13\"></a>\n* There are total 200 training example without any label to indicate which customer belong which group\n* We are going to use annual income and spending score to find the clusters in data. Note that spending score is from 1 to 100 which is assigned by the mall based on customer behavior and spending nature ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.scatter(df['Annual Income (k$)'],df['Spending Score (1-100)'])\nplt.xlabel('Annual Income')\nplt.ylabel('Spending Score')\nplt.title('Unlabelled Mall Customer Data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since we are going to use Annual Income and Spending Score  columns only, lets create 2D array of these columns for further use\nX = df.iloc[:, [3,4]].values\nX[:5] # Show first 5 records only","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Choosing The Number of Clusters <a id =\"14\"></a>\nBy visual inspection of above scatter plot, we can identify 5 possible clusters. But since there is no other information available its very difficult say it with 100% confidence. So lets try to verify this with Elbow method technique.\n\n### Elbow Method <a id =\"15\"></a>\n* Using the elbow method to find the optimal number of clusters. Let's use 1 to 11 as range of clusters.\n* We will use 'random' initialization method for this study.\n* Note that Sklearn K-Means algorithm also have ‘k-means++’ initialization method. It selects initial cluster centers for k-mean clustering in a smart way to speed up convergence.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clustering_score = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'random', random_state = 42)\n    kmeans.fit(X)\n    clustering_score.append(kmeans.inertia_) # inertia_ = Sum of squared distances of samples to their closest cluster center.\n    \n\nplt.figure(figsize=(10,6))\nplt.plot(range(1, 11), clustering_score)\nplt.scatter(5,clustering_score[4], s = 200, c = 'red', marker='*')\nplt.title('The Elbow Method')\nplt.xlabel('No. of Clusters')\nplt.ylabel('Clustering Score')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"From above elbow plot its clear that clustering scores slows down after 5 number of clusters. So we can use K= 5 for further analysis.\n\n## Compute K-Means Clustering <a id =\"16\"></a>\nCompute cluster centers and predict cluster index for each sample. Since K=5 we will get the cluster index from 0 to 4 for every data point in our dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans= KMeans(n_clusters = 5, random_state = 42)\n\n# Compute k-means clustering\nkmeans.fit(X)\n\n# Compute cluster centers and predict cluster index for each sample.\npred = kmeans.predict(X)\n\npred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'pred' contains the values index( 0 to 4) cluster for every training example. Let's add it to original dataset for better understanding.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Cluster'] = pd.DataFrame(pred, columns=['cluster'] )\nprint('Number of data points in each cluster= \\n', df['Cluster'].value_counts())\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualization <a id =\"17\"></a>\nLet's plot the centroid and cluster with different colors to visualize, how K-Means algorithm has grouped the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.scatter(X[pred == 0, 0], X[pred == 0, 1], c = 'brown', label = 'Cluster 0')\nplt.scatter(X[pred == 1, 0], X[pred == 1, 1], c = 'green', label = 'Cluster 1')\nplt.scatter(X[pred == 2, 0], X[pred == 2, 1], c = 'blue', label = 'Cluster 2')\nplt.scatter(X[pred == 3, 0], X[pred == 3, 1], c = 'purple', label = 'Cluster 3')\nplt.scatter(X[pred == 4, 0], X[pred == 4, 1], c = 'orange', label = 'Cluster 4')\n\nplt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:, 1],s = 300, c = 'red', label = 'Centroid', marker='*')\n\nplt.xlabel('Annual Income')\nplt.ylabel('Spending Score')\nplt.legend()\nplt.title('Customer Clusters')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inner Working <a id =\"18\"></a>\nUsing below code we can visualize the inner working of K-Means algorithm.\n\n* To start with we will define the random centroids. You can see in below plot that initial centroids with original data without any clusters.\n* In step 1 we will run the K-Means algorithm only for one iteration and plot the new position of centroid. Notice how centroid position changes and clusters started to form around it.\n* In step 2, we will run the K-Means algorithm for two iterations. Notice how data points are reassigned from one cluster to another as centroid position change\n* Similarly at the end we run the K-Means algorithm for six iterations, where we get the final location of centroids and associated clusters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_k_means_progress(centroid_history,n_clusters, centroid_sets, cluster_color):\n    \"\"\"\n    This function will plot the path taken by the centroids\n    \n    I/P:\n    * centroid_history: 2D array of centroids. Each element represent the centroid coordinate. \n      If there are 5 clusters then first set contains initial cluster cordinates\n      (i.e. first 5 elements) and then k_means loop will keep appending new cluster coordinates for each iteration\n    * n_clusters: Total number of clusters to find\n    * centroid_sets: At the start we set random values as our first centroid set. K-Means loop will keep adding \n    new centroid sets to centroid_history. Since we are ploting the path of centroid locations, centroid set value \n    will be K-Means loop iteration number plus 1 for initial centroid set. \n    So its value will be from 2 to K-Means loops max iter plus 1\n    * cluster_color: Just to have same line and cluster color\n    \n    O/P: Plot the centroid path\n    \"\"\"\n    c_x = [] # To store centroid X coordinated\n    c_y=[]   # To store the centroid Y coordinates\n    for i in range(0, n_clusters):\n        cluster_index = 0\n        for j in range(0, centroid_sets):\n            c_x = np.append(c_x, centroid_history[:,0][i + cluster_index])\n            c_y = np.append(c_y, centroid_history[:,1][i + cluster_index])\n            cluster_index = cluster_index + n_clusters\n            # if there are 5 clusters then first set contains initial cluster cordinates and then k_means loop will keep appending new cluster coordinates for each iteration\n        \n        plt.plot(c_x, c_y, c= cluster_color['c_' + str(i)], linestyle='--')\n        \n        # Reset coordinate arrays to avoid continuous lines\n        c_x = []\n        c_y=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\n\n# Random Initialization of Centroids\nplt.scatter(df['Annual Income (k$)'],df['Spending Score (1-100)'])\ninitial_centroid = np.array([[10, 2], [50,100], [130,20], [50,15], [140,100]])\n\nplt.scatter(initial_centroid[:,0], initial_centroid[:, 1],s = 200, c = 'red', label = 'Random Centroid', marker='*')\nplt.xlabel('Annual Income')\nplt.ylabel('Spending Score')\nplt.legend()\nplt.title('Random Initialization of Centroids')\n\n# K-Means loop of assignment and move centroid steps\ncentroid_history = []\ncentroid_history = initial_centroid\n#\ncluster_color= {'c_0':'brown','c_1':'green','c_2':'blue','c_3':'purple','c_4':'orange'}\nn_clusters = 5\nfor i in range(1,6):\n    kmeans= KMeans(n_clusters, init= initial_centroid, n_init= 1, max_iter= i, random_state = 42)  #n_init= 1 since our init parameter is array\n    \n    # Compute cluster centers and predict cluster index for each sample\n    pred = kmeans.fit_predict(X)\n\n    plt.figure(figsize=(10,6))\n    plt.scatter(X[pred == 0, 0], X[pred == 0, 1], c = 'brown', label = 'Cluster 0')\n    plt.scatter(X[pred == 1, 0], X[pred == 1, 1], c = 'green', label = 'Cluster 1')\n    plt.scatter(X[pred == 2, 0], X[pred == 2, 1], c = 'blue', label = 'Cluster 2')\n    plt.scatter(X[pred == 3, 0], X[pred == 3, 1], c = 'purple', label = 'Cluster 3')\n    plt.scatter(X[pred == 4, 0], X[pred == 4, 1], c = 'orange', label = 'Cluster 4') \n    \n    plt.scatter(centroid_history[:,0], centroid_history[:, 1],s = 50, c = 'gray', label = 'Last Centroid', marker='x')\n    \n    plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:, 1],s = 200, c = 'red', label = 'Centroid', marker='*')\n    \n    centroid_history = np.append(centroid_history, kmeans.cluster_centers_, axis=0)\n    \n    plt.xlabel('Annual Income')\n    plt.ylabel('Spending Score')\n    plt.legend()\n    plt.title('Iteration:' + str(i) + ' Assignment and Move Centroid Step')\n    \n    centroid_sets = i + 1 # Adding one for initial set of centroids\n    plot_k_means_progress(centroid_history,n_clusters, centroid_sets, cluster_color)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inner Working: GIF <a id =\"19\"></a>\n\nIsn't a GIF makes a K-Means clustering visualization even more satisfying!\n\n\n![K_means_Clustering](https://raw.githubusercontent.com/satishgunjal/images/master/K_means_Clustering.gif)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}