{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exploring Multi-Armed Bandit Problem: Epsilon-Greedy, Epsilon-Decreasing, UCB, and Thompson Sampling\n\nThe multi-armed bandit problem is a classic dilemma in the realm of sequential decision-making under uncertainty. It finds applications in a wide array of domains, from recommendation systems and clinical trials to online advertising and resource allocation. At its core, the problem revolves around efficiently allocating resources (such as time, money, or clicks) to a set of options, referred to as \"arms\" in order to maximize cumulative rewards.\n\n## What is the Multi-Armed Bandit Problem?\n\nThe multi-armed bandit problem embodies several key characteristics:\n\n### Stateless:\nIn the context of this problem, we operate in a stateless environment. This means that each decision made at any given time does not depend on prior decisions or states. It's as if we are pulling levers on a row of slot machines (the \"bandits\") independently.\n\n### No Change of Environment:\nUnlike some decision-making scenarios where the environment can change or adapt over time, the multi-armed bandit problem (in general) assumes a static environment. The probabilities of receiving rewards from each arm do not change during the course of the problem. Thus, the challenge lies in discovering the arm that consistently yields the highest average reward.\n\n### Objective:\nThe primary objective in the multi-armed bandit problem is to identify and exploit the arm that, on average, produces the largest reward. This objective necessitates a careful balance between exploring the arms to gather information and exploiting the current best-known arm to maximize rewards.\n\n## Two Kinds of Environments Explored in the Context of the Multi-Armed Bandit Problem\n\nThere are two two fundamental types of reward environments we can solve as the Multi-Armed Bandit Problem.\n\n### Discrete Reward:\nIn the discrete reward environment, each arm provides rewards from a finite set of possibilities. This discrete nature simplifies the problem to some extent but still presents interesting challenges in optimizing decision-making. An example of this type of reward is a click by a viewer of Internet Advertisement, i.e. 1 if a user clicks it while 0 if he/she does not.\n\n### Continuous Reward:\nThe continuous reward environment extends the problem to situations where arms yield rewards from a continuous range of values. This adds complexity, as decisions now involve real-valued rewards and demand more sophisticated strategies. An example of this type of reward is a a user's level of interaction or satisfaction with the recommended content by a recommendation system. It will be high (continuous measurement) when the recommended contents match with the user's interests and increase the engagement. \n\nIn this Jupyter notebook, we delve into the multi-armed bandit problem by focusing primarily on the discrete reward environment, which is a fundamental and commonly encountered scenario. \n\n## Algorithms We Will Learn Through This Notebook\n\nTo tackle the multi-armed bandit problem effectively, we will learn several well-established algorithms, each with its own approach to balancing exploration and exploitation. These algorithms include:\n\n### Epsilon Greedy Algorithm:\nThe epsilon-greedy algorithm is a straightforward approach that balances exploration (randomly choosing an arm) and exploitation (choosing the arm with the highest estimated reward). It introduces an exploration parameter, epsilon $\\epsilon$, which governs the trade-off between these two aspects.\n\n### Epsilon Decreasing:\nBuilding upon the epsilon-greedy algorithm, epsilon-decreasing gradually reduces the exploration rate over time. This strategy allows for more exploration in the initial stages and transitions toward greater exploitation as the algorithm gathers more information.\n\n### Upper Confidence Bound (UCB):\nThe UCB algorithm calculates an upper confidence bound for each arm's reward estimate, choosing the arm with the highest upper bound. This technique leverages uncertainty in reward estimates to guide exploration while favoring arms that are likely to be optimal.\n\n### Thompson Sampling:\nThompson Sampling is a probabilistic algorithm that models each arm's reward distribution. It samples from these distributions to make decisions, naturally balancing exploration and exploitation by adapting to the observed rewards.\n\nThroughout this notebook, our primary focus will be on these algorithms applied to the discrete reward environment. By the end, you will have a comprehensive understanding of how to approach and address this intriguing problem in the context of discrete rewards. So let's start!\n\n\n## Table of contents\n* [Prepare a discrete reward multi-armed bandit environment](#Preparation)\n* [Epsilon Greedy](#EG)\n* [Epsilon Decreasing](#ED)\n* [Upper Confident bound](#UCB)\n* [Thompson Sampling](#TS)\n","metadata":{}},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"code","source":"# Import packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\n\n# Parameters\nfontsize = 15\nnum_trial = 1000\nnum_ite = 100\nnum_arm = 10\narm_labels_str =  [str(x) for x in range(num_arm)]\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T12:40:30.025327Z","iopub.execute_input":"2023-10-03T12:40:30.025665Z","iopub.status.idle":"2023-10-03T12:40:30.406282Z","shell.execute_reply.started":"2023-10-03T12:40:30.025637Z","shell.execute_reply":"2023-10-03T12:40:30.405131Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"Preparation\"></a>\n# Prepare a discrete reward multi-armed bandit environment","metadata":{}},{"cell_type":"markdown","source":"To begin, we set up a discrete reward multi-armed bandit environment. In this environment, we create ten arms, each with a mean reward probability that is uniformly sampled from the range [0, 1]. Each arm yields a reward of 1 with a probability equal to its mean reward probability and a reward of 0 with a probability of 1 minus the mean probability.\n","metadata":{}},{"cell_type":"code","source":"# Initialize the bandit environment\nnum_arms = 10  # Number of arms\nrandom_seed = 2023\nnp.random.seed(random_seed)\n\n# Generate random reward probabilities for each arm\nreward_probs = np.random.uniform(0, 1, num_arms)\n\n# Find the index of the arm with the highest reward probability\nbest_arm_index = np.argmax(reward_probs)\n\n# Create labels for each arm\narm_labels = [str(x) for x in range(num_arms)]\n\n# Create a bar plot to visualize the reward probabilities of each arm\nplt.figure(figsize=(10, 5))\nbarlist = plt.bar(arm_labels, reward_probs, color='gray')\nbarlist[best_arm_index].set_color('r')\nplt.xlabel('Arm index', fontsize=fontsize)\nplt.ylabel('Reward probability', fontsize=fontsize)\n\n# Print the best and mean reward probabilities\nbest_reward_prob = reward_probs[best_arm_index]\nmean_reward_prob = np.mean(reward_probs)\nprint(f\"The best reward probability is {np.round(best_reward_prob, 2)}\")\nprint(f\"The mean reward probability is {np.round(mean_reward_prob, 2)}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T12:40:30.408316Z","iopub.execute_input":"2023-10-03T12:40:30.408935Z","iopub.status.idle":"2023-10-03T12:40:30.74085Z","shell.execute_reply.started":"2023-10-03T12:40:30.408904Z","shell.execute_reply":"2023-10-03T12:40:30.73977Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The above plot visualizes the mean probabilities associated with each arm. If we keep choosing arm1, we will receive a reward in 89% of the trials and this is the best performance we can achieve. \n\nIf we randomly explore an arm in each trial, what will be the probability of obtaining a reward? This will serve as our baseline performance against which we can later compare the performance of other algorithms. Let's implement this algorithm. We will also implement utility functions for running simulations.\n","metadata":{}},{"cell_type":"code","source":"class MultiArmedBandit:\n    def __init__(self, num_arm, reward_probs):\n        # Initialize the multi-armed bandit with the number of arms, reward probabilities, and an optional parameter.\n        self.num_arm = num_arm\n        self.reward_probs = reward_probs\n                \n    def run_iteration(self, iters, time_steps):\n        rewards_hists = []\n        explored_cnts = []\n\n        # Run multi-armed bandit simulation for a specified number of iterations.\n        for i in range(iters):\n            # Store reward history and total explored count of each arm for each iteration i.\n            reward_hist, explored_cnt = self.run_episode(time_steps)\n            rewards_hists.append(reward_hist)\n            explored_cnts.append(explored_cnt)            \n        \n        # Compute the average and standard deviation over iterations.\n        avg_reward_hist = np.mean(rewards_hists, axis=0)\n        std_reward_hist = np.std(rewards_hists, axis=0)\n        avg_explored_cnt = np.mean(explored_cnts, axis=0)\n        std_explored_cnt = np.std(explored_cnts, axis=0)\n\n        return avg_reward_hist, std_reward_hist, avg_explored_cnt, std_explored_cnt\n        \n    def run_episode(self, time_steps):\n        rewards = []\n        exp_reward_prob = np.zeros(self.num_arm)\n        explored_cnt = np.zeros(self.num_arm)\n        \n        # Perform multiple trials in a single episode.\n        for trial in range(time_steps):\n            arm, reward = self.step()\n            exp_reward_prob, explored_cnt = self.update(arm, reward, exp_reward_prob, explored_cnt)\n            rewards.append(reward)\n        return rewards, explored_cnt\n\n    def step(self):\n        # Randomly choose one arm and observe its reward.\n        arm = np.random.choice(range(self.num_arm))\n        reward = np.random.choice([0, 1], p=[1-self.reward_probs[arm], self.reward_probs[arm]])\n        return arm, reward\n        \n    def update(self, arm, reward, exp_reward_prob, explored_cnt):\n        # Update the estimated reward probability for the chosen arm.\n        explored_cnt[arm] += 1\n        exp_reward_prob[arm] = exp_reward_prob[arm] + (1/explored_cnt[arm] * (reward - exp_reward_prob[arm]))\n        return exp_reward_prob, explored_cnt\n    \n    def plot_algorithm_performance(self, algo, avg_reward_hist, std_reward_hist, avg_explored_cnt, time_steps, best_arm_prob):\n        # Create subplots to visualize algorithm performance metrics.\n        plt.figure(figsize=(20,5))\n\n        # Plot average reward over time steps.\n        plt.subplot(1,3,1)\n        plt.plot(list(range(time_steps)), avg_reward_hist, color='b')\n        plt.axhline(y=best_arm_prob, color='r')\n        plt.ylim([0, 1])\n        plt.xlabel('Time steps', fontsize=fontsize)\n        plt.ylabel('Avg. reward at each time step', fontsize=fontsize)\n\n        # Plot average reward across all time steps.\n        plt.subplot(1,3,2)\n        avg_reward = [np.mean(avg_reward_hist), best_arm_prob]\n        std_reward = [np.std(avg_reward_hist), 0]\n        plt.bar([algo, 'Best'], avg_reward, yerr=std_reward, color='gray')\n        plt.ylim([0, 1])\n        plt.xticks(fontsize=fontsize)\n        plt.ylabel('Avg. reward over time steps', fontsize=fontsize)\n\n        # Plot average explored count for each arm.\n        plt.subplot(1,3,3)\n        arm_labels =  [str(x) for x in range(self.num_arm)]\n        plt.bar(arm_labels, avg_explored_cnt, color='gray')\n        plt.xticks(fontsize=fontsize)\n        plt.ylabel('Avg. explored trial count', fontsize=fontsize)\n        ","metadata":{"execution":{"iopub.status.busy":"2023-10-03T12:40:30.741944Z","iopub.execute_input":"2023-10-03T12:40:30.742413Z","iopub.status.idle":"2023-10-03T12:40:30.7554Z","shell.execute_reply.started":"2023-10-03T12:40:30.742387Z","shell.execute_reply":"2023-10-03T12:40:30.754682Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The \"step\" function within this class implements a random exploration algorithm. Now, we will conduct 1000 trials across 100 episodes and assess the algorithm's average performance.","metadata":{}},{"cell_type":"code","source":"# Baseline: Random Exploration\nre = MultiArmedBandit(num_arm, reward_probs)\n\n# Run random exploration simulation\navg_rewards_re, std_rewards_re, avg_explored_cnt_re, std_explored_cnt_re = re.run_iteration(num_ite, num_trial)\n\n# Plot random exploration performance\nre.plot_algorithm_performance('Random', avg_rewards_re, std_rewards_re, avg_explored_cnt_re, num_trial, reward_probs[best_arm_index])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T12:40:30.756967Z","iopub.execute_input":"2023-10-03T12:40:30.757843Z","iopub.status.idle":"2023-10-03T12:40:35.828156Z","shell.execute_reply.started":"2023-10-03T12:40:30.757816Z","shell.execute_reply":"2023-10-03T12:40:35.827014Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The average reward probility of random exploration is about 0.44, which is the mean reward probability across all arms. From next section, we will explore different algorithms to solve the multi-armed bandit problem. We would like to make sure that the average reward probability of those algorithms is better than this value.\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"EG\"></a>\n# Epsilon Greedy Algorithm: Choose the most greedy option with probability 1-epsilon and choose randomly with probability epsilon","metadata":{}},{"cell_type":"markdown","source":"The multi-armed bandit problem poses a challenge: how can we maximize our rewards when faced with multiple options, each with an unknown probability of success? One straightforward approach is to try each option once and then focus on those that seem most promising based on our initial observations. For example, if we receive rewards [0, 0, 0, 0, 0, 1, 1, 1, 1, 1] from arms 0 to 9 in our first attempts, it might seem logical to randomly select one of the last 5 arms for subsequent trials. \n\nHowever, there's a catch. By doing this, we risk ignoring the first 5 arms, which yielded no rewards in the initial round. Their expected reward probabilities remain at zero, while the last 5 arms have higher expected probabilities due to their early successes. In such cases, we may miss out on arms with potentially higher rewards, like arm1 in our example.\n\nThe Epsilon Greedy Algorithm offers a solution by introducing a hyperparameter called epsilon (ε). This parameter balances exploitation (choosing the arm with the highest expected reward) and exploration (trying different arms to discover their potential). \n\nLet's dive into implementing the Epsilon Greedy Algorithm.","metadata":{}},{"cell_type":"code","source":"class EpsilonGreedy(MultiArmedBandit):\n    def __init__(self, num_arm, reward_probs, epsilon):\n        # Initialize the Epsilon Greedy algorithm with the number of arms, reward probabilities, and epsilon.\n        super().__init__(num_arm, reward_probs)\n        self.epsilon = epsilon\n\n    def run_episode(self, time_steps):\n        rewards = []\n        exp_reward_prob = np.zeros(self.num_arm)\n        explored_cnt = np.zeros(self.num_arm)\n\n        # Perform multiple trials in a single episode.\n        for trial in range(time_steps):\n            # Use exp_reward_prob to decide the next arm.\n            arm, reward = self.step(exp_reward_prob)\n            exp_reward_prob, explored_cnt = self.update(arm, reward, exp_reward_prob, explored_cnt)\n            rewards.append(reward)\n        return rewards, explored_cnt\n\n    def step(self, exp_reward_prob):\n        p = np.random.random()  # Generate a random number in the range (0, 1).\n        if p < self.epsilon:  # Exploration: Choose a random arm.\n            arm = np.random.choice(self.num_arm)\n        else:  # Exploitation: Choose the arm with the highest expected reward (break ties randomly).\n            greedy_arm_indices = np.where(exp_reward_prob == exp_reward_prob.max())[0]\n            arm = np.random.choice(greedy_arm_indices)\n        reward = np.random.choice([0, 1], p=[1 - self.reward_probs[arm], self.reward_probs[arm]])\n        return arm, reward\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T12:40:35.8297Z","iopub.execute_input":"2023-10-03T12:40:35.830331Z","iopub.status.idle":"2023-10-03T12:40:35.839668Z","shell.execute_reply.started":"2023-10-03T12:40:35.830295Z","shell.execute_reply":"2023-10-03T12:40:35.838757Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epsilon = 0.1\neg = EpsilonGreedy(num_arm, reward_probs, epsilon)\navg_rewards_eg, std_rewards_eg, avg_explored_cnt_eg, std_explored_cnt_eg = eg.run_iteration(num_ite, num_trial)\neg.plot_algorithm_performance('Epsilon Greedy', avg_rewards_eg, std_rewards_eg, avg_explored_cnt_eg, num_trial, reward_probs[best_arm_index])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T12:40:35.840686Z","iopub.execute_input":"2023-10-03T12:40:35.841127Z","iopub.status.idle":"2023-10-03T12:40:41.16565Z","shell.execute_reply.started":"2023-10-03T12:40:35.841103Z","shell.execute_reply":"2023-10-03T12:40:41.164516Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As the number of time steps increases, we observe a gradual rise in the average reward, which eventually stabilizes around the highest achievable reward value. This suggests that the Epsilon Greedy algorithm effectively identifies the best-performing arm among the ten available options. This trend is clearly evident in the right panel, where the count of explored trials is maximized for the arm with the highest reward probability (i.e. arm1). Furthermore, as we see in the middle plot, the average reward over time significantly outperforms that of the random algorithm, averaging around 0.44.\n\nIn this simulation, we set the epsilon value to 0.1. However, choosing the optimal hyperparameter value for epsilon is crucial. To address this, we will conduct simulations using various epsilon values to find the most effective choice.","metadata":{}},{"cell_type":"code","source":"epsilons = [0, 0.01, 0.1, 0.3, 0.5, 1]\n\n# Initialize lists to store results.\navg_rewards_hist_eg = []\navg_rewards_overall_eg = []\n\n# Loop through different epsilon values.\nfor epsilon in epsilons:\n    eg = EpsilonGreedy(num_arm, reward_probs, epsilon)\n    avg_rewards_eg, std_rewards_eg, avg_explored_cnt_eg, std_explored_cnt_eg = eg.run_iteration(num_ite, num_trial)\n    avg_rewards_hist_eg.append(avg_rewards_eg)\n    avg_rewards_overall_eg.append(np.mean(avg_rewards_eg))\n\n# Create a figure for visualization.\nplt.figure(figsize=(20, 5))\n\n# Plot average reward over time steps for each epsilon value.\nplt.subplot(1, 2, 1)\nfor i in range(len(epsilons)):\n    plt.plot(list(range(num_trial)), avg_rewards_hist_eg[i], label=epsilons[i])\nplt.legend(ncol=3)\nplt.axhline(y=reward_probs[best_arm_index], color='r')\nplt.xlabel('Time steps', fontsize=fontsize)\nplt.ylabel('Avg. reward at each time step', fontsize=fontsize)\n\n# Plot average reward across all time steps for each epsilon value.\nplt.subplot(1, 2, 2)\nepsilons_str = [str(x) for x in epsilons]\nplt.bar(epsilons_str, avg_rewards_overall_eg, color='gray')\nplt.ylim([0, 1])\nplt.xticks(fontsize=fontsize)\nplt.ylabel('Avg. reward over time steps', fontsize=fontsize)\n\n# Identify the best-performing epsilon value.\nbest_param_index = np.where(avg_rewards_overall_eg == np.max(avg_rewards_overall_eg))[0][0]\nprint(f\"The best parameter set is {epsilons_str[best_param_index]} with avg.reward={np.round(avg_rewards_overall_eg[best_param_index], 3)}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T12:40:41.16745Z","iopub.execute_input":"2023-10-03T12:40:41.168108Z","iopub.status.idle":"2023-10-03T12:41:09.067555Z","shell.execute_reply.started":"2023-10-03T12:40:41.168071Z","shell.execute_reply":"2023-10-03T12:41:09.066395Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In fact, an epsilon value of 0.1 yielded the highest average reward at each time step and over time steps. In the context of the Epsilon Greedy algorithm, an epsilon of 1 corresponds to pure random exploration, while an epsilon of 0 represents a purely greedy approach. Our exploration of different epsilon values serves the purpose of determining the optimal balance between exploitation and exploration.\n\nAnother noteworthy observation is that there remains a gap in the average reward between the best-performing Epsilon Greedy approach and the best arm reward. This discrepancy arises because the Epsilon Greedy algorithm continues to explore with a probability of epsilon, even after identifying the optimal arm. Ideally, we would prefer higher epsilon values at the beginning to promote exploration and gradually reduce epsilon's value to emphasize exploitation and maximize our rewards. The Epsilon Decreasing algorithm, which we will learn next, offers a solution to address this issue.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"ED\"></a>\n# Epsilon Decreasing Algorithm: Gradually reducing exploration\n","metadata":{}},{"cell_type":"markdown","source":"The Epsilon Decreasing algorithm introduces an additional hyperparameter, alpha $\\alpha$, which takes a value within the range 0 < $\\alpha$ < 1. \n\n$\\alpha$ is used to scale epsilon at each time step, gradually reducing its value over time. This approach controls a balance between exploration and exploitation, emphasizing exploration early on and favoring exploitation as we learn more.\n\nLet's see the implementation of this algorithm.","metadata":{}},{"cell_type":"code","source":"class EpsilonDecreasing(MultiArmedBandit):\n    def __init__(self, num_arm, reward_probs, epsilon, alpha):\n        # Initialize the Epsilon Decreasing algorithm with the number of arms, reward probabilities, epsilon,\n        # and alpha for gradual epsilon reduction.\n        super().__init__(num_arm, reward_probs)\n        self.epsilon = epsilon  # Original value of epsilon\n        self.epsilon_update = epsilon  # Gradually reduce the value of this epsilon\n        self.alpha = alpha\n    \n    def run_episode(self, time_steps):\n        rewards = []\n        exp_reward_prob = np.zeros(self.num_arm)\n        explored_cnt = np.zeros(self.num_arm)\n        self.epsilon_update = self.epsilon  # Initialize the value of epsilon at the beginning of each episode\n        \n        # Draw an arm, observe reward, and update predictions for a specified number of trials.\n        for trial in range(time_steps):\n            # Use exp_reward_prob to decide the next arm.\n            arm, reward = self.step(exp_reward_prob)\n            exp_reward_prob, explored_cnt = self.update(arm, reward, exp_reward_prob, explored_cnt)\n            rewards.append(reward)\n        return rewards, explored_cnt\n    \n    def step(self, exp_reward_prob):\n        p = np.random.random()  # Generate a random number in the range (0, 1).\n        if p < self.epsilon_update:  # Exploration: Choose a random arm.\n            arm = np.random.choice(self.num_arm)\n        else:  # Exploitation: Choose the arm with the highest expected reward (break ties randomly).\n            greedy_arm_indices = np.where(exp_reward_prob == exp_reward_prob.max())[0]\n            arm = np.random.choice(greedy_arm_indices)\n        \n        # Generate a reward for the chosen arm.\n        reward = np.random.choice([0, 1], p=[1 - self.reward_probs[arm], self.reward_probs[arm]])\n        \n        # Reduce the value of epsilon at each time step using alpha.\n        self.epsilon_update = self.alpha * self.epsilon_update\n        return arm, reward\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T12:41:09.069579Z","iopub.execute_input":"2023-10-03T12:41:09.069972Z","iopub.status.idle":"2023-10-03T12:41:09.081388Z","shell.execute_reply.started":"2023-10-03T12:41:09.069945Z","shell.execute_reply":"2023-10-03T12:41:09.080225Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, let's explore the behavior of the Epsilon Decreasing algorithm by examining different combinations of epsilon and alpha values. As we covered, epsilon determines the initial exploration rate, while alpha dictates the rate at which epsilon decreases over time.\nLet's visualize how epsilon changes under different epsilon and alpha combinations.\n","metadata":{}},{"cell_type":"code","source":"epsilons = [0.1, 0.5]\nalphas = [0.99, 0.995, 0.999]\nepsilons_hist = []\nparam_labels = []\n\nfor epsilon in epsilons:\n    for alpha in alphas:\n        epsilon_hist = []\n        epsilon_updated = epsilon\n        for i in range(num_trial):\n            epsilon_updated = epsilon_updated * alpha\n            epsilon_hist.append(epsilon_updated)\n        epsilons_hist.append(epsilon_hist)\n        param_labels.append(f\"{epsilon}x{alpha}\")\n\nplt.figure(figsize=(10,5))\nfor i in range(len(param_labels)):\n    plt.plot(list(range(num_trial)), epsilons_hist[i], label=param_labels[i])\nplt.legend(ncol=2)\nplt.xlabel('Time step', fontsize=fontsize)\nplt.ylabel('Epsilon', fontsize=fontsize)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T12:41:09.082819Z","iopub.execute_input":"2023-10-03T12:41:09.083914Z","iopub.status.idle":"2023-10-03T12:41:09.441657Z","shell.execute_reply.started":"2023-10-03T12:41:09.083875Z","shell.execute_reply":"2023-10-03T12:41:09.440492Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For instance, consider epsilon=0.1 and alpha=0.99 (depicted by the blue line). Here, the algorithm rapidly transitions towards almost full exploitation after approximately 300 trials because the epsilon value approaches zero. Conversely, if we start with a higher epsilon, such as 0.5, it encourages a more extended period of exploration before eventually converging into a strong exploitation strategy. People tned to use use higher epsilon values with Epsilon Decreasing compared to Epsilon Greedy. This design encourages substantial exploration in the initial phases, gradually shifting towards increased exploitation as the number of trials increases.","metadata":{}},{"cell_type":"markdown","source":"So what is the best combination of epsilon and alpha in this setup? As we did for epsilon greedy, let's run hypperparameter search.","metadata":{}},{"cell_type":"code","source":"epsilons = [0.1, 0.5]\nalphas = [0.99, 0.995, 0.999]\navg_rewards_hist_ed = []  # To store average rewards over time for different combinations.\navg_rewards_overall_ed = []  # To store overall average rewards for different combinations.\n\n# Run simulations for all combinations of epsilon x alpha.\nfor epsilon in epsilons:\n    for alpha in alphas:\n        ed = EpsilonDecreasing(num_arm, reward_probs, epsilon, alpha)\n        avg_rewards_ed, std_rewards_ed, avg_explored_cnt_ed, std_explored_cnt_ed = ed.run_iteration(num_ite, num_trial)\n        avg_rewards_hist_ed.append(avg_rewards_ed)\n        avg_rewards_overall_ed.append(np.mean(avg_rewards_ed))\n    \n# Create a figure for visualization.\nplt.figure(figsize=(20, 5))\n\n# Plot average reward over time steps for each combination of epsilon x alpha.\nplt.subplot(1, 2, 1)\nlabels = []\nplot_index = 0\nfor i in range(len(epsilons)):\n    for j in range(len(alphas)):\n        plt.plot(list(range(num_trial)), avg_rewards_hist_ed[plot_index], label=f\"{epsilons[i]}x{alphas[j]}\")\n        labels.append(f\"{epsilons[i]}x{alphas[j]}\")\n        plot_index += 1\n        \nplt.legend(ncol=2)\nplt.axhline(y=reward_probs[best_arm_index], color='r')\nplt.xlabel('Time steps', fontsize=fontsize)\nplt.ylabel('Avg. reward at each time step', fontsize=fontsize)\n\n# Plot overall average reward for each combination of epsilon x alpha.\nplt.subplot(1, 2, 2)\nplt.bar(labels, avg_rewards_overall_ed, color='gray')\nplt.ylim([0, 1])\nplt.xticks(fontsize=fontsize, rotation=90)\nplt.ylabel('Avg. reward over time steps', fontsize=fontsize)\n\n# Identify the best-performing combination of epsilon x alpha.\nbest_param_index = np.where(avg_rewards_overall_ed == np.max(avg_rewards_overall_ed))[0][0]\nprint(f\"The best parameter set is {labels[best_param_index]} with avg.reward={np.round(avg_rewards_overall_ed[best_param_index], 3)}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T12:41:09.445483Z","iopub.execute_input":"2023-10-03T12:41:09.446252Z","iopub.status.idle":"2023-10-03T12:41:38.414922Z","shell.execute_reply.started":"2023-10-03T12:41:09.446224Z","shell.execute_reply":"2023-10-03T12:41:38.413878Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In this case, the parameter set {epsilon=0.5 and alpha=0.99} is the best-performing configuration (although this outcome may slightly vary depending on the simulated values). As expected, the average reward achieved with this optimal parameter set outperforms that of the Epsilon Greedy algorithm (which was about 0.81).\n\nHowever, it's essential to note that we now have an additional hyperparameter to fine-tune in this algorithm. While Epsilon Decreasing outperforms Epsilon Greedy, optimizing it requires increased computational resources to determine the most suitable hyperparameter values.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"UCB\"></a>\n# Upper Confidence Bound (UCB): Explore uncertain arms more frequently","metadata":{}},{"cell_type":"markdown","source":"So far, we have been randomly choosing arms during exploration trials. However, is this truly an effective strategy? Ideally, we would like to explore the options where we have a higher chance of observing good results rather than wasting time on options that are likely to perform poorly.\n\nThe Upper Confidence Bound (UCB) algorithm offers a solution to this problem. Instead of relying on random exploration, UCB prioritizes options with high uncertainty, as these have the potential to yield excellent results. In UCB, there is no clear distinction between exploitation and exploration trials; arms are selected based on the following equation:\n\n$$\n\\text{UCB}(a) = \\text{Q}(a) + \\sqrt{\\frac{2\\ln(t)}{N(a)}}\n$$\n\nIn this equation, the first term represents the expected mean reward Q(a), while the second term quantifies the uncertainty associated with the option. The variable t represents the total number of trials conducted so far, and N(a) represents the number of times arm a has been selected. The second term in the equation will be relatively large for an arm that has been explored fewer times compared to the total number of trials. This encourages the algorithm to prioritize the exploration of arms with higher uncertainty.\n\nNow, let's implement this algorithm.","metadata":{}},{"cell_type":"code","source":"class UpperConfidenceBound(MultiArmedBandit):\n    def __init__(self, num_arm, reward_probs, c):\n        self.num_arm = num_arm\n        self.reward_probs = reward_probs\n        self.c = c\n    \n    def run_episode(self, time_steps):\n        rewards = []\n        exp_reward_prob = np.zeros(self.num_arm)\n        explored_cnt = np.zeros(self.num_arm)\n        \n        # Draw an arm, observe reward, and update predictions for a specified number of time steps\n        for trial in range(time_steps):\n            # Use exp_reward_prob to decide the next arm\n            arm, reward = self.step(exp_reward_prob, explored_cnt, trial)\n            exp_reward_prob, explored_cnt = self.update(arm, reward, exp_reward_prob, explored_cnt)\n            rewards.append(reward)\n        return rewards, explored_cnt\n    \n    def step(self, exp_reward_prob, explored_cnt, trial):\n        if trial < self.num_arm:  # Explore all arms once\n            arm = trial\n        else:  # UCB\n            # ucb = exp_reward_prob + self.c * np.sqrt(np.log(trial + 1) / (explored_cnt + 1))  # Use trial + 1\n            ucb = exp_reward_prob + self.c * np.sqrt(np.log(trial) / (explored_cnt))  # Use trial + 1\n            best_ucb_arm_indices = np.where(ucb == ucb.max())[0]\n            arm = np.random.choice(best_ucb_arm_indices)  # Break a tie randomly\n        reward = np.random.choice([0, 1], p=[1 - self.reward_probs[arm], self.reward_probs[arm]])\n        return arm, reward\n    ","metadata":{"execution":{"iopub.status.busy":"2023-10-03T12:41:38.416031Z","iopub.execute_input":"2023-10-03T12:41:38.416907Z","iopub.status.idle":"2023-10-03T12:41:38.425201Z","shell.execute_reply.started":"2023-10-03T12:41:38.416879Z","shell.execute_reply":"2023-10-03T12:41:38.424148Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Note that in the step function, we initially iterate through all arms once to prevent a zero value in the logarithmic term and division by 0.\n\nThe parameter 'c' is a hyperparameter for the UCB algorithm and serves to balance exploitation and exploration. A larger 'c' value leads to more exploration, while 'c=0' corresponds to a purely greedy algorithm.\n\nLet's explore the best hyperparameter 'c'.","metadata":{}},{"cell_type":"code","source":"cs = [0.1, 0.25, 0.5, 1, 2]\navg_rewards_hist_ucb = []\navg_rewards_overall_ucb = []\n\n# Run simulation with different value of 'c'\nfor c in cs:\n    ucb = UpperConfidenceBound(num_arm, reward_probs, c)\n    avg_rewards_ucb, std_rewards_ucb, avg_explored_cnt_ucb, std_explored_cnt_ucb = ucb.run_iteration(num_ite, num_trial)\n    avg_rewards_hist_ucb.append(avg_rewards_ucb)\n    avg_rewards_overall_ucb.append(np.mean(avg_rewards_ucb))\n    \n\nplt.figure(figsize=(20, 5))\nplt.subplot(1, 2, 1)\nlabels = []\n\n# Plot average rewards for each 'c'\nfor i in range(len(cs)):\n    plt.plot(list(range(num_trial)), avg_rewards_hist_ucb[i], label=f\"{cs[i]}\")\n    labels.append(f\"{cs[i]}\")\n\nplt.legend(ncol=2)\nplt.axhline(y=reward_probs[best_arm_index], color='r')\nplt.xlabel('Time steps', fontsize=fontsize)\nplt.ylabel('Avg. reward at each time step', fontsize=fontsize)\n\nplt.subplot(1, 2, 2)\n\n# Create a bar chart to display overall average rewards for each 'c'\nplt.bar(labels, avg_rewards_overall_ucb, color='gray')\nplt.ylim([0, 1])\nplt.xticks(fontsize=fontsize, rotation=90)\nplt.ylabel('Avg. reward over time steps', fontsize=fontsize)\n\n# Find the best parameter based on the highest overall average reward\nbest_param_index = np.where(avg_rewards_overall_ucb == np.max(avg_rewards_overall_ucb))[0][0]\nprint(f\"The best parameter is {labels[best_param_index]} with avg. reward={np.round(avg_rewards_overall_ucb[best_param_index], 3)}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T12:41:38.426896Z","iopub.execute_input":"2023-10-03T12:41:38.427311Z","iopub.status.idle":"2023-10-03T12:42:05.493824Z","shell.execute_reply.started":"2023-10-03T12:41:38.427273Z","shell.execute_reply":"2023-10-03T12:42:05.492766Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"With the best parameter, we observe further improvement from epsilon decreasing!","metadata":{}},{"cell_type":"markdown","source":"<a id=\"TS\"></a>\n# Thompson Sampling: Use Bayesian framework to guide the exploitation and exploration","metadata":{}},{"cell_type":"markdown","source":"Lastly, I will introduce an algorithm using Bayesian framework - Thompson sampling. Thompson sampling balances the exploration of uncertain options and the exploitation of known high-reward options using Bayesian posterior. It updates a probability distribution over the reward probabilities of different arms (posterior) and uses this distribution to make decisions.","metadata":{}},{"cell_type":"markdown","source":"Here's a high-level overview of how Thompson Sampling works:\n\n1. Initialization: Initially, the algorithm assigns a reward probability distribution (often a Beta distribution) to each arm. The distribution represents the uncertainty or belief about the true underlying reward probability of each arm.\n\n2. Arm Selection: For each time step, the algorithm samples a value from each reward probability distribution. These sampled values represent hypothetical reward probabilities for each arm.\n\n3. Exploration vs. Exploitation: Thompson Sampling then selects the arm with the highest sampled value, thus favoring actions with a higher probability of yielding a high reward. However, since the sampled values are probabilistic, Thompson Sampling naturally explores other options as well.\n\n4. Updating Probability Distributions: After observing the actual reward of the chosen arm, Thompson Sampling updates the probability distribution for that arm. This update incorporates the observed reward and adjusts the belief about the arm's true reward probability.\n\n5. Repeat: The algorithm repeats steps 2-4 for a predefined number of rounds or until a stopping criterion is met.","metadata":{}},{"cell_type":"markdown","source":"Let's implement and test this algorithm.","metadata":{}},{"cell_type":"code","source":"class ThompsonSampling(MultiArmedBandit):\n    def __init__(self, num_arm, reward_probs, prior=None):\n        # Initialize Thompson Sampling with the number of arms, reward probabilities, and an optional prior\n        self.num_arm = num_arm\n        self.reward_probs = reward_probs\n        self.prior = prior\n    \n    def run_episode(self, time_steps):\n        rewards = []\n        \n        # Initialize success-failure counts for each arm\n        if self.prior is None:\n            succ_fail = np.zeros([self.num_arm, 2])\n        else:\n            succ_fail = [x[:] for x in self.prior]  # Deep copy of the prior\n        explored_cnt = np.zeros(self.num_arm)\n        \n        # Draw an arm, observe reward, and update predictions for a specified number of time steps\n        for trial in range(time_steps):\n            arm, reward = self.step(succ_fail, explored_cnt)\n            succ_fail, explored_cnt = self.update(arm, reward, succ_fail, explored_cnt)\n            rewards.append(reward)\n        \n        return rewards, explored_cnt\n    \n    def step(self, succ_fail, explored_cnt):\n        # Action selection: sample expected reward probability for each arm from posterior (Beta distribution)\n        samples = [np.random.beta(s + 1, f + 1) for s, f in succ_fail]\n        arm = np.argmax(samples)\n        \n        # Observe reward\n        reward = np.random.choice([0, 1], p=[1 - self.reward_probs[arm], self.reward_probs[arm]])\n        \n        return arm, reward\n    \n    def update(self, arm, reward, succ_fail, explored_cnt):\n        # Increment the exploration count for the selected arm\n        explored_cnt[arm] += 1\n        \n        # Update alpha (success count) and beta (failure count) in Beta distribution to update the posterior\n        if reward == 0:  # No reward trial\n            succ_fail[arm][1] += 1\n        elif reward == 1:  # Reward trial\n            succ_fail[arm][0] += 1\n        \n        return succ_fail, explored_cnt\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T12:42:05.495212Z","iopub.execute_input":"2023-10-03T12:42:05.495478Z","iopub.status.idle":"2023-10-03T12:42:05.504626Z","shell.execute_reply.started":"2023-10-03T12:42:05.495454Z","shell.execute_reply":"2023-10-03T12:42:05.503575Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ts = ThompsonSampling(num_arm, reward_probs)\navg_rewards_ts, std_rewards_ts, avg_explored_cnt_ts, std_explored_cnt_ts = ts.run_iteration(num_ite, num_trial)\nts.plot_algorithm_performance('Thompson Sampling', avg_rewards_ts, std_rewards_ts, avg_explored_cnt_ts, num_trial, reward_probs[best_arm_index])\nprint(f\"Avg.reward={np.round(np.mean(avg_rewards_ts),3)}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T12:42:05.505787Z","iopub.execute_input":"2023-10-03T12:42:05.506203Z","iopub.status.idle":"2023-10-03T12:42:12.053544Z","shell.execute_reply.started":"2023-10-03T12:42:05.506179Z","shell.execute_reply":"2023-10-03T12:42:12.052884Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"One advantage of Thompson sampling is its ability to incorporate prior knowledge. By providing a well-informed prior to the algorithm, it can learn the optimal action more efficiently. Let's examine how the choice of prior influences the algorithm's performance. In this context, I will compare three scenarios:\n\n1. Uniform prior\n2. Good prior with many successful experiences with arm1 (i.e. the best arm)\n3. Poor prior with limited successful experiences with arm1\n","metadata":{}},{"cell_type":"code","source":"prior1 = None # Uniform proir\nprior2 = [[1,10],[10,1],[1,10],[1,10],[1,10],[1,10],[1,10],[1,10],[1,10],[1,10]] # Good prior\nprior3 = [[10,1],[1,10],[10,1],[10,1],[10,1],[10,1],[10,1],[10,1],[10,1],[10,1]] # Bad prior\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T12:42:12.054442Z","iopub.execute_input":"2023-10-03T12:42:12.05496Z","iopub.status.idle":"2023-10-03T12:42:12.060194Z","shell.execute_reply.started":"2023-10-03T12:42:12.054933Z","shell.execute_reply":"2023-10-03T12:42:12.059197Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"priors = [prior1, prior2, prior3]\navg_rewards_hist_ts = []\navg_rewards_overall_ts = []\n\n# Iterate through different priors\nfor prior in priors:\n    ts = ThompsonSampling(num_arm, reward_probs, prior)\n    avg_rewards_ts, std_rewards_ts, avg_explored_cnt_ts, std_explored_cnt_ts = ts.run_iteration(num_ite, num_trial)\n    avg_rewards_hist_ts.append(avg_rewards_ts)\n    avg_rewards_overall_ts.append(np.mean(avg_rewards_ts))\n    \n\nplt.figure(figsize=(20, 5))\nplt.subplot(1, 2, 1)\nlabel_str = []\n\n# Plot average rewards for each prior\nfor i in range(len(priors)):\n    plt.plot(list(range(num_trial)), avg_rewards_hist_ts[i], label=f\"prior{i+1}\")\n    label_str.append(f\"prior{i}\")\n\nplt.legend(ncol=3)\nplt.axhline(y=reward_probs[best_arm_index], color='r')\nplt.xlabel('Time steps', fontsize=fontsize)\nplt.ylabel('Avg. reward at each time step', fontsize=fontsize)\n\nplt.subplot(1, 2, 2)\n\n# Create a bar chart to display overall average rewards for each prior\nplt.bar(label_str, avg_rewards_overall_ts, color='gray')\nplt.ylim([0, 1])\nplt.xticks(fontsize=fontsize)\nplt.ylabel('Avg. reward over time steps', fontsize=fontsize)\n\n# Find the best prior based on the highest overall average reward\nbest_param_index = np.where(avg_rewards_overall_ts == np.max(avg_rewards_overall_ts))[0][0]\nprint(f\"The best prior is {label_str[best_param_index]} with avg. reward={np.round(avg_rewards_overall_ts[best_param_index], 3)}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T12:42:12.061831Z","iopub.execute_input":"2023-10-03T12:42:12.062353Z","iopub.status.idle":"2023-10-03T12:42:26.990767Z","shell.execute_reply.started":"2023-10-03T12:42:12.062326Z","shell.execute_reply":"2023-10-03T12:42:26.989737Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A good prior (prior2) assists the algorithm in focusing on the best arm from the beginning, leading to optimal performance. In contrast, a poor prior (prior3) faces challenges in correcting initial misconceptions, resulting in a longer time to discover promising arms. This outcome underscores the significance of prior knowledge. However, it's worth noting that the algorithm performs equally well, even with a uniform prior, compared to other algorithms I've introduced thus far. Thus, Thompson sampling is useful algorithm for balancing exploitation and exploration, even in the absence of prior knowledge. Moreover, if prior knowledge is available, the algorithm's performance can be further enhanced.\n","metadata":{}},{"cell_type":"markdown","source":"# Reference","metadata":{}},{"cell_type":"markdown","source":"- Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.\n- Russo, D. J., Van Roy, B., Kazerouni, A., Osband, I., & Wen, Z. (2018). A tutorial on thompson sampling. Foundations and Trends® in Machine Learning, 11(1), 1-96.","metadata":{}}]}