{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1238325,"sourceType":"datasetVersion","datasetId":709857}],"dockerImageVersionId":29944,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"`Zoumana KEITA, Data Scientist`\n\n# Latent Dirichlet Allocation / Analysis (LDA)     \n\n**Note**: you will need to unzip the data from the `data` folder in order to follow this notebook.  \n\nThis is a probabilistic model used to find clusters assigments for documents.  \nIt uses two probability values to cluster documents: \n- **P(word | topic)**: the probability that a particular word is associated with a particular topic. This first set of probability is also considered as the **Word X Topic** matrix.  \n- **P(topics | documents)**: the topics associated with documents. This second set of probability is considered as **Topics X Documents** matrix.   \nThese probability values are calculated for all words, topics and documents.    \n\nFor this tutorial, we will be using the dataset of the Australian Broadcasting Corporation, available on kaggle:   \nhttps://www.kaggle.com/therohk/million-headlines ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true}},{"cell_type":"markdown","source":"## Import Useful Libraries ","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false,"jupyter":{"outputs_hidden":true}}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the Dataset","metadata":{}},{"cell_type":"code","source":"news_data = pd.read_csv(\"../input/news-data.csv\")\nnews_data.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"news_data.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Our data have over a million of records, and there are two columns: \n- the date a particular headline have been published.  \n- the actual headline.   \nBy looking at the first 5 rows, we can see that we don't have the topic of the headline text! So, we will use LDA to attempt to figure out clusters of the news.   \n**A million** of record, that is a lot of data. To do so, we will use only **12000** records to make the computation faster.   ","metadata":{}},{"cell_type":"markdown","source":"## Preprocessing.    ","metadata":{}},{"cell_type":"code","source":"NUM_SAMPLES = 12000 # The number of sample to use \nsample_df = news_data.sample(NUM_SAMPLES, replace=False).reset_index(drop=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We are not interested in the **publish_data** column, since we will only be using **headline_text** data.    \n\n**`max_df`**` : float in range [0.0, 1.0] or int, default=1.0`<br>\nWhen building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n\n**`min_df`**` : float in range [0.0, 1.0] or int, default=1`<br>\nWhen building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.     \n\n\nBe defining the **CountVectorizer** object as below, we ignore:   \n- all terms that occur over 95% times in our document corpus. We say in this case that the terms occuring more than this threshold are not significant, most of them are  `stopwords`.   \n\n- all the terms that occur fewer than twice in the entire corpus.  ","metadata":{}},{"cell_type":"code","source":"cv = CountVectorizer(max_df=0.95, min_df=2, stop_words=\"english\")\ndtm = cv.fit_transform(sample_df['headline_text'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dtm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can observe that our Document X Term Matrix (dtm) has:  \n- 12000 documents, and.  \n- 6506 distinct words   \n\nWe can also get all those words using the `get_feature_names()` function","metadata":{}},{"cell_type":"code","source":"feature_names = cv.get_feature_names()\nlen(feature_names) # show the total number of distinct words","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's have a look at some of the features that have been extracted from the documents.  ","metadata":{}},{"cell_type":"code","source":"feature_names[6500:]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LDA     \nFrom our DTM matrix, we can now build our LDA to extract topics from the underlined texts. The number of topic to be extracted is a hyperparameter, so we do not know it a a glance. In our case, we will be using 7 topics.   \nLDA is an iterative algorithm, we will have 30 iterations in our case, but the default value is 10.  ","metadata":{}},{"cell_type":"code","source":"NUM_TOPICS = 7 \nLDA_model = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=30, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LDA_model.fit(dtm)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Show Stored Words.   \nLet's randomnly have a look at some words of that have been stored.  ","metadata":{}},{"cell_type":"code","source":"len(feature_names)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random \nfor index in range(15):\n    random_word_ID = random.randint(0, 6506)\n    print(cv.get_feature_names()[random_word_ID])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Top Words Per Topic","metadata":{}},{"cell_type":"code","source":"len(LDA_model.components_[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pick a single topic \na_topic = LDA_model.components_[0]\n\n# Get the indices that would sort this array\na_topic.argsort()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# The word least representative of this topic\na_topic[597]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# The word most representative of this topic\na_topic[3598]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let have a look at the top 10 words for the topic we previously took","metadata":{}},{"cell_type":"code","source":"top_10_words_indices = a_topic.argsort()[-10:]\n\nfor i in top_10_words_indices:\n    print(cv.get_feature_names()[i])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This looks like Government Article. Let's have a look at all the 7 topics found. ","metadata":{}},{"cell_type":"code","source":"for i, topic in enumerate(LDA_model.components_):\n    print(\"THE TOP {} WORDS FOR TOPIC #{}\".format(10, i))\n    print([cv.get_feature_names()[index] for index in topic.argsort()[-10:]])\n    print(\"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Attach Discovered Topic Labels to Original News","metadata":{}},{"cell_type":"code","source":"final_topics = LDA_model.transform(dtm)\nfinal_topics.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**final_topics** contains, for each of our 12000 documents, the probability score of how likely a document belongs to each of the 7 topics.  This is a Document X Topics matrix. \nFor example, below is the probability values for the first document.","metadata":{}},{"cell_type":"code","source":"final_topics[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_topics[0].argmax()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This value (4) means that our LDA model thinks that the first document belongs to the 4th topic.","metadata":{}},{"cell_type":"markdown","source":"### Combination with the original data     \nLet's create a new column called **Topic N°** that will correspond to the topic value to which each document belongs to.","metadata":{}},{"cell_type":"code","source":"sample_df[\"Topic N°\"] = final_topics.argmax(axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"According to our LDA model:   \n- the first document belongs to 4th topic.  \n- the second document belongs to 4th topic. \n- the third document belongs to 6th topic.  \netc.   ","metadata":{}},{"cell_type":"markdown","source":"## Some Visualization       \nWe will be using the `pyldavis` module to visualize the topics associated to our documents.   ","metadata":{}},{"cell_type":"code","source":"import pyLDAvis.sklearn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pyLDAvis.enable_notebook() # To enable the visualization on the notebook","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"panel = pyLDAvis.sklearn.prepare(LDA_model, dtm, cv, mds='tsne') # Create the panel for the visualization\npanel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Some Comments On The Graphic     \n\n- By selecting a particular term on the right, we can see which topic(s) it belongs.    \n- Vice-versa, by choosing a topic on the left, we can see all the terms, from most to least relevant term.  ","metadata":{}},{"cell_type":"markdown","source":"**If you liked this kernel, please upvote. I am also open to suggestions**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}