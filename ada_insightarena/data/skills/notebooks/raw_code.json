{
  "code_cells": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for data visualization purposes\nimport seaborn as sns # for statistical data visualization\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.",
    "import warnings\n\nwarnings.filterwarnings('ignore')",
    "data = '/kaggle/input/adult-dataset/adult.csv'\n\ndf = pd.read_csv(data, header=None, sep=',\\s')",
    "# view dimensions of dataset\n\ndf.shape",
    "# preview the dataset\n\ndf.head()",
    "col_names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship',\n             'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income']\n\ndf.columns = col_names\n\ndf.columns",
    "# let's again preview the dataset\n\ndf.head()",
    "# view summary of dataset\n\ndf.info()",
    "# find categorical variables\n\ncategorical = [var for var in df.columns if df[var].dtype=='O']\n\nprint('There are {} categorical variables\\n'.format(len(categorical)))\n\nprint('The categorical variables are :\\n\\n', categorical)",
    "# view the categorical variables\n\ndf[categorical].head()",
    "# check missing values in categorical variables\n\ndf[categorical].isnull().sum()",
    "# view frequency counts of values in categorical variables\n\nfor var in categorical: \n    \n    print(df[var].value_counts())",
    "# view frequency distribution of categorical variables\n\nfor var in categorical: \n    \n    print(df[var].value_counts()/np.float(len(df)))",
    "# check labels in workclass variable\n\ndf.workclass.unique()",
    "# check frequency distribution of values in workclass variable\n\ndf.workclass.value_counts()",
    "# replace '?' values in workclass variable with `NaN`\n\n\ndf['workclass'].replace('?', np.NaN, inplace=True)",
    "# again check the frequency distribution of values in workclass variable\n\ndf.workclass.value_counts()",
    "# check labels in occupation variable\n\ndf.occupation.unique()\n",
    "# check frequency distribution of values in occupation variable\n\ndf.occupation.value_counts()",
    "# replace '?' values in occupation variable with `NaN`\n\ndf['occupation'].replace('?', np.NaN, inplace=True)\n",
    "# again check the frequency distribution of values in occupation variable\n\ndf.occupation.value_counts()",
    "# check labels in native_country variable\n\ndf.native_country.unique()\n",
    "# check frequency distribution of values in native_country variable\n\ndf.native_country.value_counts()\n",
    "# replace '?' values in native_country variable with `NaN`\n\ndf['native_country'].replace('?', np.NaN, inplace=True)",
    "# again check the frequency distribution of values in native_country variable\n\ndf.native_country.value_counts()",
    "df[categorical].isnull().sum()",
    "# check for cardinality in categorical variables\n\nfor var in categorical:\n    \n    print(var, ' contains ', len(df[var].unique()), ' labels')",
    "# find numerical variables\n\nnumerical = [var for var in df.columns if df[var].dtype!='O']\n\nprint('There are {} numerical variables\\n'.format(len(numerical)))\n\nprint('The numerical variables are :', numerical)",
    "# view the numerical variables\n\ndf[numerical].head()",
    "# check missing values in numerical variables\n\ndf[numerical].isnull().sum()",
    "X = df.drop(['income'], axis=1)\n\ny = df['income']",
    "# split X and y into training and testing sets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "# check the shape of X_train and X_test\n\nX_train.shape, X_test.shape",
    "# check data types in X_train\n\nX_train.dtypes",
    "# display categorical variables\n\ncategorical = [col for col in X_train.columns if X_train[col].dtypes == 'O']\n\ncategorical",
    "# display numerical variables\n\nnumerical = [col for col in X_train.columns if X_train[col].dtypes != 'O']\n\nnumerical",
    "# print percentage of missing values in the categorical variables in training set\n\nX_train[categorical].isnull().mean()",
    "# print categorical variables with missing data\n\nfor col in categorical:\n    if X_train[col].isnull().mean()>0:\n        print(col, (X_train[col].isnull().mean()))",
    "# impute missing categorical variables with most frequent value\n\nfor df2 in [X_train, X_test]:\n    df2['workclass'].fillna(X_train['workclass'].mode()[0], inplace=True)\n    df2['occupation'].fillna(X_train['occupation'].mode()[0], inplace=True)\n    df2['native_country'].fillna(X_train['native_country'].mode()[0], inplace=True)    ",
    "# check missing values in categorical variables in X_train\n\nX_train[categorical].isnull().sum()",
    "# check missing values in categorical variables in X_test\n\nX_test[categorical].isnull().sum()",
    "# check missing values in X_train\n\nX_train.isnull().sum()",
    "# check missing values in X_test\n\nX_test.isnull().sum()",
    "# print categorical variables\n\ncategorical",
    "X_train[categorical].head()",
    "# import category encoders\n\nimport category_encoders as ce",
    "# encode remaining variables with one-hot encoding\n\nencoder = ce.OneHotEncoder(cols=['workclass', 'education', 'marital_status', 'occupation', 'relationship', \n                                 'race', 'sex', 'native_country'])\n\nX_train = encoder.fit_transform(X_train)\n\nX_test = encoder.transform(X_test)",
    "X_train.head()",
    "X_train.shape",
    "X_test.head()",
    "X_test.shape",
    "cols = X_train.columns",
    "from sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\n\nX_train = scaler.fit_transform(X_train)\n\nX_test = scaler.transform(X_test)\n",
    "X_train = pd.DataFrame(X_train, columns=[cols])",
    "X_test = pd.DataFrame(X_test, columns=[cols])",
    "X_train.head()",
    "# train a Gaussian Naive Bayes classifier on the training set\nfrom sklearn.naive_bayes import GaussianNB\n\n\n# instantiate the model\ngnb = GaussianNB()\n\n\n# fit the model\ngnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)\n\ny_pred",
    "from sklearn.metrics import accuracy_score\n\nprint('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))",
    "y_pred_train = gnb.predict(X_train)\n\ny_pred_train",
    "print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))",
    "# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(gnb.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(gnb.score(X_test, y_test)))",
    "# check class distribution in test set\n\ny_test.value_counts()",
    "# check null accuracy score\n\nnull_accuracy = (7407/(7407+2362))\n\nprint('Null accuracy score: {0:0.4f}'. format(null_accuracy))",
    "# Print the Confusion Matrix and slice it into four pieces\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred)\n\nprint('Confusion matrix\\n\\n', cm)\n\nprint('\\nTrue Positives(TP) = ', cm[0,0])\n\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\n\nprint('\\nFalse Positives(FP) = ', cm[0,1])\n\nprint('\\nFalse Negatives(FN) = ', cm[1,0])",
    "# visualize confusion matrix with seaborn heatmap\n\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')",
    "from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))",
    "TP = cm[0,0]\nTN = cm[1,1]\nFP = cm[0,1]\nFN = cm[1,0]",
    "# print classification accuracy\n\nclassification_accuracy = (TP + TN) / float(TP + TN + FP + FN)\n\nprint('Classification accuracy : {0:0.4f}'.format(classification_accuracy))\n",
    "# print classification error\n\nclassification_error = (FP + FN) / float(TP + TN + FP + FN)\n\nprint('Classification error : {0:0.4f}'.format(classification_error))\n",
    "# print precision score\n\nprecision = TP / float(TP + FP)\n\n\nprint('Precision : {0:0.4f}'.format(precision))\n",
    "recall = TP / float(TP + FN)\n\nprint('Recall or Sensitivity : {0:0.4f}'.format(recall))",
    "true_positive_rate = TP / float(TP + FN)\n\n\nprint('True Positive Rate : {0:0.4f}'.format(true_positive_rate))",
    "false_positive_rate = FP / float(FP + TN)\n\n\nprint('False Positive Rate : {0:0.4f}'.format(false_positive_rate))",
    "specificity = TN / (TN + FP)\n\nprint('Specificity : {0:0.4f}'.format(specificity))",
    "# print the first 10 predicted probabilities of two classes- 0 and 1\n\ny_pred_prob = gnb.predict_proba(X_test)[0:10]\n\ny_pred_prob",
    "# store the probabilities in dataframe\n\ny_pred_prob_df = pd.DataFrame(data=y_pred_prob, columns=['Prob of - <=50K', 'Prob of - >50K'])\n\ny_pred_prob_df",
    "# print the first 10 predicted probabilities for class 1 - Probability of >50K\n\ngnb.predict_proba(X_test)[0:10, 1]",
    "# store the predicted probabilities for class 1 - Probability of >50K\n\ny_pred1 = gnb.predict_proba(X_test)[:, 1]",
    "# plot histogram of predicted probabilities\n\n\n# adjust the font size \nplt.rcParams['font.size'] = 12\n\n\n# plot histogram with 10 bins\nplt.hist(y_pred1, bins = 10)\n\n\n# set the title of predicted probabilities\nplt.title('Histogram of predicted probabilities of salaries >50K')\n\n\n# set the x-axis limit\nplt.xlim(0,1)\n\n\n# set the title\nplt.xlabel('Predicted probabilities of salaries >50K')\nplt.ylabel('Frequency')",
    "# plot ROC Curve\n\nfrom sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred1, pos_label = '>50K')\n\nplt.figure(figsize=(6,4))\n\nplt.plot(fpr, tpr, linewidth=2)\n\nplt.plot([0,1], [0,1], 'k--' )\n\nplt.rcParams['font.size'] = 12\n\nplt.title('ROC curve for Gaussian Naive Bayes Classifier for Predicting Salaries')\n\nplt.xlabel('False Positive Rate (1 - Specificity)')\n\nplt.ylabel('True Positive Rate (Sensitivity)')\n\nplt.show()\n",
    "# compute ROC AUC\n\nfrom sklearn.metrics import roc_auc_score\n\nROC_AUC = roc_auc_score(y_test, y_pred1)\n\nprint('ROC AUC : {:.4f}'.format(ROC_AUC))",
    "# calculate cross-validated ROC AUC \n\nfrom sklearn.model_selection import cross_val_score\n\nCross_validated_ROC_AUC = cross_val_score(gnb, X_train, y_train, cv=5, scoring='roc_auc').mean()\n\nprint('Cross validated ROC AUC : {:.4f}'.format(Cross_validated_ROC_AUC))",
    "# Applying 10-Fold Cross Validation\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(gnb, X_train, y_train, cv = 10, scoring='accuracy')\n\nprint('Cross-validation scores:{}'.format(scores))",
    "# compute Average cross-validation score\n\nprint('Average cross-validation score: {:.4f}'.format(scores.mean()))"
  ]
}