{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Purpose\n1. Understand textrank: an application of pagerank algorithm on text\n\n<table align=\"left\">\n  <td>\n    <a href=\"https://colab.research.google.com/github/Ankush-Chander/graph-notebooks/blob/main/notebooks/textrank_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://www.kaggle.com/code/latebloomer/pagerank-in-nlp\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n  </td>\n</table>","metadata":{}},{"cell_type":"markdown","source":"# spaCy\nspaCy is a library for **advanced Natural Language Processing in Python and Cython**. It's built on the very latest research, and was designed from day one to be used in real products.\n\nspaCy comes with **pretrained pipelines and currently supports tokenization and training for 70+ languages**. It features state-of-the-art speed and neural network models for **tagging, parsing, named entity recognition, text classification** and more, multi-task learning with pretrained transformers like BERT, as well as a production-ready training system and easy model packaging, deployment and workflow management. spaCy is commercial open-source software, released under the MIT license.","metadata":{}},{"cell_type":"markdown","source":"## installation","metadata":{}},{"cell_type":"code","source":"! pip install spacy\n! pip install networkx\n! pip install pytextrank\n! pip install icecream\n! python -m spacy download en_core_web_sm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Usage","metadata":{}},{"cell_type":"code","source":"import networkx as nx\nimport matplotlib.pyplot as plt\nimport spacy\nfrom icecream import ic \nnlp = spacy.load(\"en_core_web_sm\")\n\n\ntext = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.\"\ndoc = nlp(text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"| Feature | Description |\n| --- | --- |\n| Linguistic annotations | Insights into a text's grammatical structure, including word types and how the words are related to each other. |\n| Sentence boundary detection | Finding and segmenting individual sentences. |\n| Tokenization | The process of segmenting text into words, punctuation, and so on. |\n| Part-of-speech tags and dependencies | spaCy can parse and tag a given Doc, making predictions of which tag or label most likely applies in this context. |\n| Named Entities | spaCy can recognize various types of named entities in a document, such as person, country, product, or book title. |\n| Word vectors and similarity | Similarity is determined by comparing word vectors or \"word embeddings\", multi-dimensional meaning representations of a word. |","metadata":{}},{"cell_type":"code","source":"for sent in doc.sents:\n    ic(sent.start, sent.end, sent.text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenization and linguistic properties\nfor token in doc[:5]:\n    ic(token.idx, token.text, token.lemma_,  token.pos_, token.tag_, token.is_alpha, token.is_stop)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# noun phrase detection\nfor nc in doc.noun_chunks:\n    ic(nc.start, nc.end, nc.text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Textrank algorithm\nTextRank is a graph based unsupervised technique for keyphrase extraction and extractive summarization.  \nTo enable the application of graph-based ranking algorithms to natural language texts, we have to\nbuild a graph that represents the text, and interconnects words or other text entities with meaningful relations.  \n\nReference: [Mihalcea, Rada, and Paul Tarau. \"Textrank: Bringing order into text.\" Proceedings of the 2004 conference on empirical methods in natural language processing. 2004.](https://aclanthology.org/W04-3252.pdf)","metadata":{}},{"cell_type":"markdown","source":"## Create a textgraph\n1. **Identify text units that best define the task at hand, and add them as vertices in the graph.**  \n   In our case we will consider (lemma, pos_) as nodes. For eg: `('compatibility', 'NOUN')` will be our node.\n2. **Identify relations that connect such text units, and use these relations to draw edges between vertices in the graph. Edges can be directed or undirected, weighted or unweighted.**  \nWe will consider two nodes to be connected if they lie in a window of 3 within the same sentence\n3. **Iterate the graph-based ranking algorithm until convergence.**   \n   We use `nx.pagerank` to calculate the pagerank scores of each node in textgraph\n5. **Sort vertices based on their final score. Use the values attached to each vertex for ranking/selection decisions.**\n","metadata":{}},{"cell_type":"code","source":"\ndef increment_edge (graph, node0, node1):\n    if graph.has_edge(node0, node1):\n        graph[node0][node1][\"weight\"] += 1.0\n    else:\n        graph.add_edge(node0, node1, weight=1.0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def link_sentence(doc, sent, lemma_graph, seen_lemma):\n    visited_tokens = []\n    visited_nodes = []\n\n    for token in sent:\n        # token = doc[i]\n        # filter tokens based on allowed pos tags only\n        if token.pos_ not in POS_KEPT:\n            continue\n        \n        key = (token.lemma_, token.pos_)\n\n        if key not in seen_lemma:\n            seen_lemma[key] = set([token.i])\n        else:\n            seen_lemma[key].add(token.i)\n\n        node_id = list(seen_lemma.keys()).index(key)\n\n        if not node_id in lemma_graph:\n            lemma_graph.add_node(node_id)\n\n        for prev_token in range(len(visited_tokens) - 1, -1, -1):\n            \n            if token.i - visited_tokens[prev_token] <= 3:\n                print(f\"add edge [{token.lemma_}]<===>[{doc[visited_tokens[prev_token]].lemma_}]\")\n                increment_edge(lemma_graph, node_id, visited_nodes[prev_token])\n            else:\n                break\n\n        visited_tokens.append(token.i)\n        visited_nodes.append(node_id)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"POS_KEPT = [\"ADJ\", \"NOUN\", \"PROPN\", \"VERB\"]\n\nlemma_graph = nx.Graph()\nseen_lemma = {}\nfor sent in doc.sents:\n    link_sentence(doc, sent, lemma_graph, seen_lemma)\n\n\nprint(f\"lemma_graph created with {len(lemma_graph.nodes)} nodes and {len(lemma_graph.edges)} edges.\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id2text = {}\nkeys = list(seen_lemma.keys())\n\nfor i in range(len(seen_lemma)):\n    id2text[i] = keys[i][0].lower()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calculate pagerank","metadata":{}},{"cell_type":"code","source":"# calculate pagerank\nlemma_scores = nx.pagerank(lemma_graph)\nlemma_scores = {node: round(score,3) for node, score in lemma_scores.items()}\n# print sorted scores\n{id2text[key]: val for key, val in sorted(lemma_scores.items(), key=lambda x: x[1], reverse=True)}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# draw graph with size of nodes as per pagerank score\nfig = plt.figure(figsize=(9, 9))\npos = nx.spring_layout(lemma_graph)\nnx.draw(lemma_graph, pos=pos, with_labels=False, font_weight=\"bold\", node_size=[v * 10000 for v in lemma_scores.values()])\nnode_labels = nx.draw_networkx_labels(lemma_graph, pos, id2text)\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Keyphrase extraction\nTo calculate phrase rank:\n1. we take sum of ranks of constituent lemmas.\n$$\nrank\\_sum = \\sum_{i=1}^{n} {rank_i}\n$$\n2. use root mean square (RMS) to normalize the contributions of all the tokens.  \n$$\n    \\text{phrase\\_rank} = \\sqrt{\\frac{rank\\_sum}{\\text{chunk\\_len} + \\text{non\\_lemma}}}\n$$\n4. Discount it based on tokens that are not present in lemma graph \n$$\n    \\text{non\\_lemma\\_discount} = \\frac{\\text{chunk\\_len}}{\\text{chunk\\_len} + (2.0 * non\\_lemma) + 1.0} \n$$\n$$\n    \\text{phrase\\_rank} = \\text{phrase\\_rank} * \\text{non\\_lemma\\_discount}\n$$","metadata":{}},{"cell_type":"code","source":"import math\n\nranks = lemma_scores\n\ndef collect_phrases (chunk, phrases, counts):\n    chunk_len = chunk.end - chunk.start\n    sq_sum_rank = 0.0\n    non_lemma = 0\n    compound_key = set([])\n\n    for token in chunk:\n        key = (token.lemma_, token.pos_)\n\n        if key in seen_lemma:\n            node_id = list(seen_lemma.keys()).index(key)\n            rank = ranks[node_id]\n            sq_sum_rank += rank\n            compound_key.add(key)\n        else:\n            non_lemma += 1\n\n    # although the noun chunking is greedy, we discount the ranks using a\n    # point estimate based on the number of non-lemma tokens within a phrase\n    non_lemma_discount = chunk_len / (chunk_len + (2.0 * non_lemma) + 1.0)\n\n    # use root mean square (RMS) to normalize the contributions of all the tokens\n    phrase_rank = math.sqrt(sq_sum_rank / (chunk_len + non_lemma))\n    phrase_rank *= non_lemma_discount\n\n    # remove spurious punctuation\n    phrase = chunk.text.lower().replace(\"'\", \"\")\n\n    # create a unique key for the the phrase based on its lemma components\n    compound_key = tuple(sorted(list(compound_key)))\n\n    if not compound_key in phrases:\n        phrases[compound_key] = set([ (phrase, phrase_rank) ])\n        counts[compound_key] = 1\n    else:\n        phrases[compound_key].add( (phrase, phrase_rank) )\n        counts[compound_key] += 1\n\n    ic(phrase_rank, chunk.text, chunk_len, counts[compound_key])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"phrases = {}\ncounts = {}\n\nfor chunk in doc.noun_chunks:\n    collect_phrases(chunk, phrases, counts)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Variants of textrank","metadata":{}},{"cell_type":"markdown","source":"### Base pytextrank","metadata":{}},{"cell_type":"code","source":"import spacy\nimport pytextrank\n\n# example text\ntext = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.\"\n\n# load a spaCy model, depending on language, scale, etc.\nnlp = spacy.load(\"en_core_web_sm\")\n\n# add PyTextRank to the spaCy pipeline\nnlp.add_pipe(\"textrank\")\ndoc = nlp(text)\n\n# examine the top-ranked phrases in the document\nfor phrase in doc._.phrases[:10]:\n    print(phrase.text)\n    print(phrase.rank, phrase.count)\n    print(phrase.chunks)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Biased textrank\nis a graph-based content extraction method inspired by the popular TextRank algorithm that ranks text spans according to their importance for language processing tasks and according to their relevance to an input 'focus'. Biased TextRank enables focused content extraction for text by modifying the random restarts in the execution of TextRank.\n\nReferences: [kazemi-etal-2020-biased](https://derwen.ai/docs/ptr/biblio/#kazemi-etal-2020-biased)","metadata":{}},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")\nnlp.add_pipe(\"biasedtextrank\");\n\ntext = \"\"\"Chelsea 'opted against' signing Salomon Rondón on deadline day.\nChelsea reportedly opted against signing Salomón Rondón on deadline day despite their long search for a new centre forward. With Olivier Giroud expected to leave, the Blues targeted Edinson Cavani, Dries Mertens and Moussa Dembele – only to end up with none of them. According to Telegraph Sport, Dalian Yifang offered Rondón to Chelsea only for them to prefer keeping Giroud at the club. Manchester United were also linked with the Venezuela international before agreeing a deal for Shanghai Shenhua striker Odion Ighalo. Manager Frank Lampard made no secret of his transfer window frustration, hinting that to secure top four football he ‘needed’ signings. Their draw against Leicester on Saturday means they have won just four of the last 13 Premier League matches.\n\"\"\"\n\ndoc = nlp(text)\n\nfocus = \"Leicester\"\ndoc._.textrank.change_focus(focus,bias=10.0,  default_bias=0.0)\n\nfor phrase in doc._.phrases[:10]:\n    ic(phrase)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PositionRank \nis an unsupervised model for keyphrase extraction from scholarly documents that incorporates information from all positions of a word’s occurrences into a biased PageRank.\nSpecifically, we propose to assign a higher probability to a word found on the 2nd position as compared with a word found on the 50th position in the same document.\nThe weight of each candidate word is equal to its inverse position in the document. If the same word appears multiple times in the target document, then we sum all its position weights.","metadata":{}},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")\nnlp.add_pipe(\"positionrank\");\ntext = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.\"\ndoc = nlp(text)\n\nfor phrase in doc._.phrases[:10]:\n    ic(phrase) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TopicRank\nis a graph-based keyphrase extraction method that relies on a topical representation of the document. Candidate keyphrases are clustered into topics and used as vertices in a complete graph. A graph-based ranking model is applied to assign a significance score to each topic. Keyphrases are then generated by selecting a candidate from each of the topranked topics.\n\nReference: [bougouin-etal-2013-topicrank](https://derwen.ai/docs/ptr/biblio/#bougouin-etal-2013-topicrank)","metadata":{}},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")\nnlp.add_pipe(\"topicrank\");\n\ntext = \"\"\"Chelsea 'opted against' signing Salomon Rondón on deadline day.\nChelsea reportedly opted against signing Salomón Rondón on deadline day despite their long search for a new centre forward. With Olivier Giroud expected to leave, the Blues targeted Edinson Cavani, Dries Mertens and Moussa Dembele – only to end up with none of them. According to Telegraph Sport, Dalian Yifang offered Rondón to Chelsea only for them to prefer keeping Giroud at the club. Manchester United were also linked with the Venezuela international before agreeing a deal for Shanghai Shenhua striker Odion Ighalo. Manager Frank Lampard made no secret of his transfer window frustration, hinting that to secure top four football he ‘needed’ signings. Their draw against Leicester on Saturday means they have won just four of the last 13 Premier League matches.\n\"\"\"\ndoc = nlp(text)\n\nfor phrase in doc._.phrases[:10]:\n    ic(phrase.text)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n1. [Spacy-101](https://spacy.io/usage/spacy-101)\n2. [TextRank: Bringing Order into Texts](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)\n3. [pytextrank- github](https://github.com/DerwenAI/pytextrank)\n4. [pytextrank - tutorial](https://derwen.ai/docs/ptr/explain_algo/)\n5. [PyTextRank: A Python Implementation of NLP Text Rank Algorithm by Paco Nathan](https://www.youtube.com/watch?v=ZwlPsdRDtMI)","metadata":{}}]}